{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.13 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import random \n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import utils\n",
    "import albumentations as albu\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import py7zr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder architecture for the segmentation model\n",
    "ENCODER = 'resnet18'\n",
    "\n",
    "# Use ImageNet pretrained weights for the encoder initialization\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Activation function applied to the output layer\n",
    "ACTIVATION = 'softmax2d'\n",
    "\n",
    "# Determine device to use (GPU if available, otherwise CPU)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Other hyperparameters\n",
    "batch_size = 2\n",
    "EPOCHS = 100\n",
    "lr = 1e-4\n",
    "INFER_HEIGHT = 640\n",
    "INFER_WIDTH = 640\n",
    "# Number of epochs after which to decrease the learning rate\n",
    "LR_DECREASE_STEP = 15\n",
    "\n",
    "# Factor by which to decrease the learning rate LR_DECREASE_STEP epochs\n",
    "LR_DECREASE_COEF = 2\n",
    "\n",
    "# step size for quality when training models\n",
    "step_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class and visualization functions\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import cv2\n",
    "CLASSES = [\n",
    "            \"background\",    # Class 0: Background\n",
    "            \"bulldozer\",     # Class 1: Bulldozer\n",
    "            \"car\",           # Class 2: Car\n",
    "            \"caterpillar\",   # Class 3: Caterpillar\n",
    "            \"crane\",         # Class 4: Crane\n",
    "            \"crusher\",       # Class 5: Crusher\n",
    "            \"driller\",       # Class 6: Driller\n",
    "            \"excavator\",     # Class 7: Excavator\n",
    "            \"human\",         # Class 8: Human\n",
    "            \"roller\",        # Class 9: Roller\n",
    "            \"tractor\",       # Class 10: Tractor\n",
    "            \"truck\"          # Class 11: Truck\n",
    "         ] \n",
    "colors_imshow = {\n",
    "    \"background\": np.array([0, 0, 0]),\n",
    "    \"bulldozer\": np.array([0, 183, 235]),\n",
    "    \"car\": np.array([255, 255, 0]),\n",
    "    \"caterpillar\": np.array([0, 16, 235]),\n",
    "    \"crane\": np.array([199, 252, 0]),\n",
    "    \"crusher\": np.array([255, 0, 140]),\n",
    "    \"driller\": np.array([14, 122, 254]),\n",
    "    \"excavator\": np.array([255, 171, 171]),\n",
    "    \"human\": np.array([254, 0, 86]),\n",
    "    \"roller\": np.array([255, 0, 255]),\n",
    "    \"tractor\": np.array([128, 128, 0]),\n",
    "    \"truck\": np.array([134, 34, 255]),\n",
    "}\n",
    "def _colorize_mask(mask: np.ndarray):\n",
    "    \"\"\"Colorizes a single-channel mask into a multichannel mask using predefined colors.\n",
    "\n",
    "    Args:\n",
    "        mask (np.ndarray): Single-channel mask where each pixel denotes a class.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Colorized mask with RGB channels representing different classes.\n",
    "        dict: Dictionary containing square ratios of each class in the mask.\n",
    "    \"\"\"\n",
    "    mask = mask.squeeze()  # Ensure the mask is squeezed to remove singleton dimensions\n",
    "    colored_mask = np.zeros((*mask.shape, 3), dtype=np.int64)  # Initialize an empty colored mask\n",
    "    square_ratios = {}  # Dictionary to store square ratios of each class\n",
    "\n",
    "    # Iterate over each class code and corresponding class name\n",
    "    for cls_code, cls in enumerate(CLASSES):\n",
    "        cls_mask = mask == cls_code  # Create a mask for the current class\n",
    "        square_ratios[cls] = cls_mask.sum() / cls_mask.size  # Calculate square ratio for the class\n",
    "        colored_mask += np.multiply.outer(cls_mask, colors_imshow[cls]).astype(np.int64)  # Colorize the mask\n",
    "\n",
    "    return colored_mask, square_ratios  # Return the colorized mask and square ratios\n",
    "\n",
    "def reverse_normalize(img, mean, std):\n",
    "    \"\"\"Reverse normalization of an image.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Normalized image.\n",
    "        mean (list): Mean values used for normalization.\n",
    "        std (list): Standard deviation values used for normalization.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Unnormalized image.\n",
    "    \"\"\"\n",
    "    img = img * np.array(std) + np.array(mean)  # Reverse normalization\n",
    "    return img  # Return the unnormalized image\n",
    "\n",
    "def visualize_predicts(img: np.ndarray, mask_gt: np.ndarray, mask_pred: np.ndarray, normalized=False):\n",
    "    \"\"\"Visualizes the original image, ground truth mask, and predicted mask.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Original image.\n",
    "        mask_gt (np.ndarray): Ground truth mask.\n",
    "        mask_pred (np.ndarray): Predicted mask.\n",
    "        normalized (bool, optional): Whether the image is normalized. Defaults to False.\n",
    "    \"\"\"\n",
    "    _, axes = plt.subplots(1, 3, figsize=(10, 5))  # Create subplots for image, GT mask, and predicted mask\n",
    "    img = img.transpose(1, 2, 0)  # Transpose image dimensions to match matplotlib's format\n",
    "\n",
    "    if normalized:\n",
    "        # Reverse the normalization to get the unnormalized image\n",
    "        img = reverse_normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    axes[0].imshow(img)  # Plot the original image\n",
    "    mask_gt, square_ratios = _colorize_mask(mask_gt)  # Colorize the ground truth mask\n",
    "    title = \"Areas:\\n\" + \"\\n\".join([f\"{cls}: {square_ratios[cls]*100:.1f}%\" for cls in CLASSES])\n",
    "    axes[1].imshow(mask_gt, cmap=\"twilight\")  # Plot the colorized ground truth mask\n",
    "    axes[1].set_title(f\"GT mask\\n\" + title)  # Set title for ground truth mask subplot\n",
    "\n",
    "    mask_pred, square_ratios = _colorize_mask(mask_pred)  # Colorize the predicted mask\n",
    "    title = \"Areas:\\n\" + \"\\n\".join([f\"{cls}: {square_ratios[cls]*100:.1f}%\" for cls in CLASSES])\n",
    "    axes[2].imshow(mask_pred, cmap=\"twilight\")  # Plot the colorized predicted mask\n",
    "    axes[2].set_title(f\"PRED mask\\n\" + title)  # Set title for predicted mask subplot\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()  # Display the plot\n",
    "\n",
    "def visualize_multichannel_mask(image, multichannel_mask, class_names):\n",
    "    # Dictionary mapping class names to RGB colors for visualization\n",
    "    colors_imshow = {\n",
    "        \"background\": np.array([0, 0, 0]),\n",
    "        \"bulldozer\": np.array([0, 183, 235]),\n",
    "        \"car\": np.array([255, 255, 0]),\n",
    "        \"caterpillar\": np.array([0, 16, 235]),\n",
    "        \"crane\": np.array([199, 252, 0]),\n",
    "        \"crusher\": np.array([255, 0, 140]),\n",
    "        \"driller\": np.array([14, 122, 254]),\n",
    "        \"excavator\": np.array([255, 171, 171]),\n",
    "        \"human\": np.array([254, 0, 86]),\n",
    "        \"roller\": np.array([255, 0, 255]),\n",
    "        \"tractor\": np.array([128, 128, 0]),\n",
    "        \"truck\": np.array([134, 34, 255]),\n",
    "    }\n",
    "\n",
    "    # Helper function to convert multichannel mask to single channel and calculate class ratios\n",
    "    def _convert_multichannel2singlechannel(mc_mask):\n",
    "        sc_mask = np.zeros((mc_mask.shape[0], mc_mask.shape[1], 3), dtype=np.int64)\n",
    "        square_ratios = {}\n",
    "\n",
    "        # Iterate over each channel in the multichannel mask\n",
    "        for i, singlechannel_mask in enumerate(mc_mask.transpose(2, 0, 1)):\n",
    "            cls = class_names[i]  # Get the class name from class_names list\n",
    "            singlechannel_mask = singlechannel_mask.squeeze()  # Remove singleton dimensions if any\n",
    "\n",
    "            # Calculate ratio of pixels belonging to the class\n",
    "            square_ratios[cls] = singlechannel_mask.sum() / singlechannel_mask.size\n",
    "\n",
    "            # Generate the single channel mask by multiplying the mask with the color and adding to sc_mask\n",
    "            sc_mask += np.multiply.outer(singlechannel_mask > 0, colors_imshow[cls]).astype(np.int64)\n",
    "\n",
    "        # Generate the title for the visualization based on class ratios\n",
    "        title = \"Areas: \" + \"\\n\".join([f\"{cls}: {square_ratios[cls]*100:.1f}%\" for cls in class_names])\n",
    "        return sc_mask, title\n",
    "\n",
    "    # Create subplots for image and mask visualization\n",
    "    _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Display the original image\n",
    "    axes[0].imshow(image)\n",
    "    \n",
    "    # Convert multichannel mask to single channel and display alongside the image\n",
    "    mask_to_show, title = _convert_multichannel2singlechannel(multichannel_mask)\n",
    "    axes[1].imshow(mask_to_show.astype(np.uint8))  # Convert mask to uint8 for display\n",
    "    axes[1].set_title(title)  # Set title displaying class ratios\n",
    "    \n",
    "    # Adjust layout for better visualization\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "# create class for dataset\n",
    "class Excavators(BaseDataset):\n",
    "    def __init__(self, images_dir, masks_dir, labels_dir, augmentation=None, preprocessing=None, compression=None):\n",
    "        \"\"\"\n",
    "        Initializes the Dataset object.\n",
    "\n",
    "        Args:\n",
    "            images_dir (str): Directory containing the input images.\n",
    "            masks_dir (str): Directory containing the corresponding masks.\n",
    "            augmentation (callable, optional): Optional augmentation function to be applied to images and masks.\n",
    "            preprocessing (callable, optional): Optional preprocessing function to be applied to images and masks.\n",
    "        \"\"\"\n",
    "        # Load and sort paths to images and masks\n",
    "        self.images_paths, self.masks_paths = self._get_sorted_paths(images_dir, masks_dir)\n",
    "        print(f\"Loaded {len(self.images_paths)} images from {images_dir}\")\n",
    "        print(f\"Loaded {len(self.masks_paths)} masks from {masks_dir}\")\n",
    "\n",
    "        # Load class colors from a file\n",
    "        self.cls_colors = self._get_classes_colors(labels_dir)\n",
    "\n",
    "        # Store augmentation and preprocessing functions\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.compression = compression\n",
    "\n",
    "    def _get_sorted_paths(self, images_dir, masks_dir):\n",
    "        \"\"\"\n",
    "        Retrieves and sorts paths to images and masks.\n",
    "\n",
    "        Args:\n",
    "            images_dir (str): Directory containing the input images.\n",
    "            masks_dir (str): Directory containing the corresponding masks.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Two lists containing sorted paths to images and masks.\n",
    "        \"\"\"\n",
    "        # Retrieve paths using glob and sort them based on filenames\n",
    "        images_paths = glob(\"{}{}{}\".format(images_dir,os.sep,\"*\"))\n",
    "        masks_paths = glob(\"{}{}{}\".format(masks_dir,os.sep,\"*\"))\n",
    "        \n",
    "        images_paths.sort(key=lambda x: os.path.basename(x))\n",
    "        masks_paths.sort(key=lambda x: os.path.basename(x))\n",
    "        \n",
    "        return images_paths, masks_paths\n",
    "\n",
    "    def _get_classes_colors(self, label_colors_dir):\n",
    "        \"\"\"\n",
    "        Loads class colors from a text file.\n",
    "\n",
    "        Args:\n",
    "            label_colors_dir (str): Path to the file containing class colors.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary mapping class names to RGB color tuples.\n",
    "        \"\"\"\n",
    "        cls_colors = {}\n",
    "        with open(label_colors_dir) as file:\n",
    "            while line := file.readline():\n",
    "                B, G, R, label = line.rstrip().split()\n",
    "                cls_colors[label] = (int(R), int(G), int(B))  # Store colors as RGB tuple\n",
    "        CLASSES = [\n",
    "            \"background\",    # Class 0: Background\n",
    "            \"bulldozer\",     # Class 1: Bulldozer\n",
    "            \"car\",           # Class 2: Car\n",
    "            \"caterpillar\",   # Class 3: Caterpillar\n",
    "            \"crane\",         # Class 4: Crane\n",
    "            \"crusher\",       # Class 5: Crusher\n",
    "            \"driller\",       # Class 6: Driller\n",
    "            \"excavator\",     # Class 7: Excavator\n",
    "            \"human\",         # Class 8: Human\n",
    "            \"roller\",        # Class 9: Roller\n",
    "            \"tractor\",       # Class 10: Tractor\n",
    "            \"truck\"          # Class 11: Truck\n",
    "        ] \n",
    "        # Order colors according to predefined class order (CLASSES)\n",
    "        keyorder = CLASSES\n",
    "        cls_colors_ordered = {}\n",
    "        for k in keyorder:\n",
    "            if k in cls_colors:\n",
    "                cls_colors_ordered[k] = cls_colors[k]\n",
    "            elif k == \"background\":\n",
    "                cls_colors_ordered[k] = (0, 0, 0)  # Black background if not specified\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected label {k}, cls colors: {cls_colors}\")\n",
    "\n",
    "        return cls_colors_ordered\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its corresponding masks from the dataset.\n",
    "\n",
    "        Args:\n",
    "            i (int): Index of the image and mask pair to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Processed image and masks as NumPy arrays.\n",
    "        \"\"\"\n",
    "        # Load image and mask from the paths\n",
    "        image_path = self.images_paths[i]\n",
    "        mask_path = self.masks_paths[i]\n",
    "        image = np.array(Image.open(image_path).convert('RGB'))\n",
    "        mask = np.array(Image.open(mask_path).convert('RGB'))\n",
    "\n",
    "        # Initialize multichannel masks\n",
    "        masks = np.zeros((mask.shape[0], mask.shape[1], len(self.cls_colors)), dtype=np.float32)\n",
    "        \n",
    "        # Generate masks for each class based on color\n",
    "        for idx, (cls_name, color) in enumerate(self.cls_colors.items()):\n",
    "            # Define lower boundary for the color (in RGB)\n",
    "            lower = np.array(color, dtype=np.uint8)\n",
    "            \n",
    "            # Create a mask for the current class\n",
    "            class_mask = np.all(mask == lower, axis=-1)\n",
    "            \n",
    "            # Convert to binary mask (0 or 1 values)\n",
    "            class_mask = class_mask.astype(np.float32)\n",
    "            \n",
    "            # Assign the mask to the corresponding channel in masks\n",
    "            masks[:, :, idx] = class_mask\n",
    "        \n",
    "        # Apply augmentations if provided\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=masks)\n",
    "            image, masks = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "        # Apply preprocessing if provided\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=masks)\n",
    "            image, masks = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "        return image, masks\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of images in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of images in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.images_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 616 images from excavator_dataset_w_masks2\\dataset\\Train\\\n",
      "Loaded 616 masks from excavator_dataset_w_masks2\\dataset\\Trainannot\\\n",
      "Loaded 116 images from excavator_dataset_w_masks2\\dataset\\Validation\\\n",
      "Loaded 116 masks from excavator_dataset_w_masks2\\dataset\\Validationannot\\\n"
     ]
    }
   ],
   "source": [
    "# create instances of classes\n",
    "# define relevant directories for the dataset class\n",
    "training_data_directory = \"{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"Train\",os.sep)\n",
    "training_masks_directory = \"{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"Trainannot\",os.sep)\n",
    "labels_directory = \"{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"label_colors.txt\")\n",
    "val_data_directory = \"{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"Validation\",os.sep)\n",
    "val_masks_directory = \"{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"Validationannot\",os.sep)\n",
    "test_data_directory = \"{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"Test\",os.sep)\n",
    "test_masks_directory = \"{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"Testannot\",os.sep)\n",
    "\n",
    "#create instances of training and validation datasets\n",
    "training_dataset = Excavators(training_data_directory, training_masks_directory, labels_directory)\n",
    "validation_dataset = Excavators(val_data_directory, val_masks_directory, labels_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    \"\"\"\n",
    "    Returns augmentation pipeline for training images.\n",
    "    \"\"\"\n",
    "    train_transform = [\n",
    "        albu.HorizontalFlip(p=0.5),  # Horizontal flip with 50% probability\n",
    "\n",
    "        albu.LongestMaxSize(max_size=INFER_HEIGHT, always_apply=True),  # Resize the longest side of the image to INFER_HEIGHT\n",
    "        albu.PadIfNeeded(min_height=int(INFER_HEIGHT*1.1), min_width=int(INFER_WIDTH*1.1), border_mode=2, always_apply=True),  # Pad the image if needed to ensure the minimum height and width\n",
    "        albu.RandomCrop(height=INFER_HEIGHT, width=INFER_WIDTH, always_apply=True),  # Randomly crop the image to INFER_HEIGHT x INFER_WIDTH\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Sharpen(alpha=(0.1, 0.2), lightness=(0.1, 0.2), p=0.5),  # Apply sharpening with probability 50%\n",
    "                albu.Blur(blur_limit=[1, 3], p=0.5),  # Apply blurring with probability 50%\n",
    "                albu.GaussNoise(var_limit=(1, 5), p=0.5),  # Apply Gaussian noise with probability 50%\n",
    "            ],\n",
    "            p=0.7,  # Probability of applying any augmentation from the OneOf list\n",
    "        ),\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),  # Random brightness/contrast adjustment with probability 50%\n",
    "                albu.HueSaturationValue(hue_shift_limit=5, sat_shift_limit=10, val_shift_limit=5, p=0.5),  # Random hue/saturation/value adjustment with probability 50%\n",
    "                albu.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.5),  # Random RGB shift with probability 50%\n",
    "            ],\n",
    "            p=0.7,  # Probability of applying any color augmentation from the OneOf list\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"\n",
    "    Returns augmentation pipeline for validation/testing images.\n",
    "    \"\"\"\n",
    "    test_transform = [\n",
    "        albu.LongestMaxSize(max_size=INFER_HEIGHT, always_apply=True),  # Resize the longest side of the image to INFER_HEIGHT\n",
    "        albu.PadIfNeeded(min_height=INFER_HEIGHT, min_width=INFER_WIDTH, border_mode=2, always_apply=True),  # Pad the image if needed to ensure height=INFER_HEIGHT and width=INFER_WIDTH\n",
    "        albu.CenterCrop(height=INFER_HEIGHT, width=INFER_WIDTH, always_apply=True),  # Crop the center of the image to INFER_HEIGHT x INFER_WIDTH\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    \"\"\"\n",
    "    Convert image/mask to tensor format.\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.ndarray): Input image or mask.\n",
    "        kwargs: Additional arguments (not used in this function).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Transposed and converted to float32 tensor.\n",
    "    \"\"\"\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"\n",
    "    Constructs preprocessing transform.\n",
    "\n",
    "    Args:\n",
    "        preprocessing_fn (callable): Data normalization function.\n",
    "\n",
    "    Returns:\n",
    "        albumentations.Compose: Preprocessing transform.\n",
    "    \"\"\"\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),  # Apply preprocessing function to image\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),  # Convert image and mask to tensor format\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function preprocess_input at 0x000002144FAE2D40>, input_space='RGB', input_range=[0, 1], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define preprocessing\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "# Classic approach: divide pixel values by 255 to normalize between [0, 1]\n",
    "# preprocessing_fn = lambda img, **kwargs: img.astype(\"float32\") / 255\n",
    "preprocessing_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for a single model\n",
    "# define training loop function\n",
    "def training_loop(model, optimizer, train_loader,val_loader, train_epoch, valid_epoch, save_path, jit_save_path):\n",
    "    #TRAININGTYME\n",
    "    max_score = 0\n",
    "\n",
    "    loss_logs = {\"train\": [], \"val\": []}\n",
    "    metric_logs = {\"train\": [], \"val\": []}\n",
    "    for i in range(0, EPOCHS):\n",
    "        \n",
    "        print('\\nEpoch: {}'.format(i))\n",
    "        train_logs = train_epoch.run(train_loader)\n",
    "        train_loss, train_metric_IOU = list(train_logs.values())\n",
    "        loss_logs[\"train\"].append(train_loss)\n",
    "        metric_logs[\"train\"].append(train_metric_IOU)\n",
    "\n",
    "        valid_logs = valid_epoch.run(val_loader)\n",
    "        val_loss, val_metric_IOU = list(valid_logs.values())\n",
    "        loss_logs[\"val\"].append(val_loss)\n",
    "        metric_logs[\"val\"].append(val_metric_IOU)\n",
    "        \n",
    "        # do something (save model, change lr, etc.)\n",
    "        if max_score < valid_logs['iou_score']:\n",
    "            max_score = valid_logs['iou_score']\n",
    "            torch.save(model, save_path)\n",
    "            # Save the model with JIT\n",
    "            # Create a tensor with the specified dimensions\n",
    "            trace_image = torch.randn(batch_size, 3, INFER_HEIGHT, INFER_WIDTH)\n",
    "            # Trace the model using the example input\n",
    "            traced_model = torch.jit.trace(model, trace_image.to(DEVICE))\n",
    "            torch.jit.save(traced_model, jit_save_path)\n",
    "            print('Model saved!')\n",
    "\n",
    "        print(\"LR:\", optimizer.param_groups[0]['lr'])\n",
    "        if i > 0 and i % LR_DECREASE_STEP == 0:\n",
    "            print('Decrease decoder learning rate')\n",
    "            optimizer.param_groups[0]['lr'] /= LR_DECREASE_COEF\n",
    "    return loss_logs, metric_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLV3_training_loop():\n",
    "    DLV3_directory = 'models/best_models_DLV3/'\n",
    "    os.makedirs(DLV3_directory, exist_ok=True)\n",
    "    for q in range (0, 101, step_size):\n",
    "        model_DLV3 = smp.DeepLabV3(encoder_name=ENCODER, encoder_weights=ENCODER_WEIGHTS, classes=len(CLASSES), activation=ACTIVATION)\n",
    "        # datasets for training\n",
    "        training_data_directory = \"{}{}{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,q,os.sep,\"Train\",os.sep)\n",
    "        training_masks_directory = \"{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"Trainannot\",os.sep)\n",
    "\n",
    "        val_data_directory = \"{}{}{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,q,os.sep,\"Validation\",os.sep)\n",
    "        val_masks_directory= \"{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"Validationannot\",os.sep)\n",
    "        labels_directory = \"{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"label_colors.txt\")\n",
    "        # Create a training dataset using images from X_TRAIN_DIR and masks from Y_TRAIN_DIR.\n",
    "        # Add augmentations defined by get_training_augmentation() and apply preprocessing defined by get_preprocessing(preprocessing_fn).\n",
    "        training_dataset = Excavators(\n",
    "            training_data_directory, \n",
    "            training_masks_directory, \n",
    "            labels_directory,\n",
    "            augmentation=get_training_augmentation(), \n",
    "            preprocessing=get_preprocessing(preprocessing_fn)\n",
    "        )\n",
    "\n",
    "        # Create a validation dataset using images from X_VALID_DIR and masks from Y_VALID_DIR.\n",
    "        # Add validation augmentations defined by get_validation_augmentation() and apply the same preprocessing function as the training dataset.\n",
    "        valid_dataset = Excavators(\n",
    "            val_data_directory, \n",
    "            val_masks_directory, \n",
    "            labels_directory,\n",
    "            augmentation=get_validation_augmentation(), \n",
    "            preprocessing=get_preprocessing(preprocessing_fn)\n",
    "        )\n",
    "\n",
    "        # create training and validation dataloaders\n",
    "        train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(valid_dataset, batch_size= 1, shuffle=False)\n",
    "        \n",
    "        # Define the criterion, metric and optimizer for training the model.\n",
    "        criterion = utils.losses.DiceLoss()\n",
    "        metric = [utils.metrics.IoU()]\n",
    "        optimizer = torch.optim.Adam([ \n",
    "            dict(params=model_DLV3.parameters(), lr=lr),  # Use Adam optimizer with initial learning rate INIT_LR\n",
    "        ])\n",
    "\n",
    "        # define train and valid epoch for 1 model\n",
    "        train_epoch = utils.train.TrainEpoch(\n",
    "            model_DLV3,                    # Model to be trained\n",
    "            loss= criterion,                # Loss function to compute during training\n",
    "            metrics=metric,          # Metrics to compute during training (e.g., F-score, IoU)\n",
    "            optimizer=optimizer,      # Optimizer used to update model parameters\n",
    "            device=DEVICE,            # Device (CPU or GPU) where the training takes place\n",
    "            verbose=True,             # Print training progress (verbose mode)\n",
    "        )\n",
    "\n",
    "        # Create an epoch runner for validation.\n",
    "        # This object encapsulates the validation loop over the samples in the dataloader.\n",
    "        valid_epoch = utils.train.ValidEpoch(\n",
    "            model_DLV3,                    # Model to be evaluated\n",
    "            loss= criterion,                # Loss function to compute during validation\n",
    "            metrics= metric,          # Metrics to compute during validation\n",
    "            device=DEVICE,            # Device (CPU or GPU) where the validation takes place\n",
    "            verbose=True,             # Print validation progress (verbose mode)\n",
    "        )\n",
    "\n",
    "        # train and define save directories for one model\n",
    "        DLV3_save_dir = \"{}{}{}{}{}{}\".format(\"models\",os.sep,\"best_models_DLV3\",os.sep,q,\".pth\")\n",
    "        DLV3_jit_save_dir = \"{}{}{}{}{}{}\".format(\"models\",os.sep,\"best_models_DLV3\",os.sep,q,\".pt\") \n",
    "        loss_logs_DLV3, metric_logs_DLV3 = training_loop(model_DLV3, optimizer, train_loader,val_loader, train_epoch, valid_epoch, DLV3_save_dir, DLV3_jit_save_dir)\n",
    "        # Plotting training and validation losses and metrics\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "        axes[0].plot(loss_logs_DLV3[\"train\"], label = \"train\")\n",
    "        axes[0].plot(loss_logs_DLV3[\"val\"], label = \"val\")\n",
    "        axes[0].set_title(f\"losses - Dice\")\n",
    "\n",
    "        axes[1].plot(metric_logs_DLV3[\"train\"], label = \"train\")\n",
    "        axes[1].plot(metric_logs_DLV3[\"val\"], label = \"val\")\n",
    "        axes[1].set_title(f\"IOU\")\n",
    "\n",
    "        # Adding a main title for both plots\n",
    "        plt.suptitle(f\"DLV3 model trained on quality = {q} \", fontsize=16)\n",
    "\n",
    "        # Adjusting layout to make room for the main title\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "        # [ax.legend() for ax in axes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 616 images from excavator_dataset_w_masks2\\dataset\\0\\Train\\\n",
      "Loaded 616 masks from excavator_dataset_w_masks2\\dataset\\Trainannot\\\n",
      "Loaded 116 images from excavator_dataset_w_masks2\\dataset\\0\\Validation\\\n",
      "Loaded 116 masks from excavator_dataset_w_masks2\\dataset\\Validationannot\\\n",
      "\n",
      "Epoch: 0\n",
      "train: 100%|██████████| 308/308 [03:45<00:00,  1.37it/s, dice_loss - 0.4391, iou_score - 0.5154] \n",
      "valid: 100%|██████████| 116/116 [00:33<00:00,  3.42it/s, dice_loss - 0.2984, iou_score - 0.6039]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\segmentation_models_pytorch\\base\\model.py:16: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if h % output_stride != 0 or w % output_stride != 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "LR: 0.0001\n",
      "\n",
      "Epoch: 1\n",
      "train:  28%|██▊       | 86/308 [01:04<02:46,  1.33it/s, dice_loss - 0.2726, iou_score - 0.6375]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run training loop\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mDLV3_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 67\u001b[0m, in \u001b[0;36mDLV3_training_loop\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m DLV3_save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m,os\u001b[38;5;241m.\u001b[39msep,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_models_DLV3\u001b[39m\u001b[38;5;124m\"\u001b[39m,os\u001b[38;5;241m.\u001b[39msep,q,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m DLV3_jit_save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m,os\u001b[38;5;241m.\u001b[39msep,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_models_DLV3\u001b[39m\u001b[38;5;124m\"\u001b[39m,os\u001b[38;5;241m.\u001b[39msep,q,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m---> 67\u001b[0m loss_logs_DLV3, metric_logs_DLV3 \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_DLV3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLV3_save_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLV3_jit_save_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Plotting training and validation losses and metrics\u001b[39;00m\n\u001b[0;32m     69\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, optimizer, train_loader, val_loader, train_epoch, valid_epoch, save_path, jit_save_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, EPOCHS):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i))\n\u001b[1;32m---> 12\u001b[0m     train_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     train_loss, train_metric_IOU \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train_logs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     14\u001b[0m     loss_logs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[1;32mc:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\segmentation_models_pytorch\\utils\\train.py:49\u001b[0m, in \u001b[0;36mEpoch.run\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     41\u001b[0m metrics_meters \u001b[38;5;241m=\u001b[39m {metric\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m: AverageValueMeter() \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics}\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\n\u001b[0;32m     44\u001b[0m     dataloader,\n\u001b[0;32m     45\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_name,\n\u001b[0;32m     46\u001b[0m     file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstdout,\n\u001b[0;32m     47\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose),\n\u001b[0;32m     48\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m iterator:\n\u001b[1;32m---> 49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 289\u001b[0m, in \u001b[0;36mExcavators.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# Apply preprocessing if provided\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing:\n\u001b[1;32m--> 289\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m     image, masks \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m], sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, masks\n",
      "File \u001b[1;32mc:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\albumentations\\core\\composition.py:302\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(data)\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m--> 302\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_data_post_transform(data)\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(data)\n",
      "File \u001b[1;32mc:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\albumentations\\core\\transforms_interface.py:115\u001b[0m, in \u001b[0;36mBasicTransform.__call__\u001b[1;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic:\n\u001b[0;32m    114\u001b[0m         kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_key][\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m deepcopy(params)\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\albumentations\\core\\transforms_interface.py:126\u001b[0m, in \u001b[0;36mBasicTransform.apply_with_params\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key2func \u001b[38;5;129;01mand\u001b[39;00m arg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     target_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key2func[key]\n\u001b[1;32m--> 126\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m arg\n",
      "File \u001b[1;32mc:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\albumentations\\augmentations\\transforms.py:2346\u001b[0m, in \u001b[0;36mLambda.apply\u001b[1;34m(self, img, **params)\u001b[0m\n\u001b[0;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, img: np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m   2345\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_apply_fns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m-> 2346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run training loop\n",
    "DLV3_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DLV3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 51 images from excavator_dataset_w_masks2\\dataset\\Test\\\n",
      "Loaded 51 masks from excavator_dataset_w_masks2\\dataset\\Testannot\\\n"
     ]
    }
   ],
   "source": [
    "# create compressed testing dataset:\n",
    "test_data_directory = \"{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"Test\",os.sep)\n",
    "test_masks_directory = \"{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"Testannot\",os.sep)\n",
    "# create dataset object\n",
    "test_dataset = Excavators(\n",
    "    test_data_directory,  # Directory containing test images\n",
    "    test_masks_directory,  # Directory containing test masks\n",
    "    labels_directory,\n",
    "    augmentation=get_validation_augmentation(),  # Apply validation-specific augmentations\n",
    "    preprocessing=get_preprocessing(preprocessing_fn)  # Preprocessing function for normalization\n",
    ")\n",
    "# create dataloader\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "# define MSE function to compare compressed and non compressed images\n",
    "def calculate_mse(original, compressed):\n",
    "    # Ensure the images have the same shape\n",
    "    if original.shape != compressed.shape:\n",
    "        raise ValueError(\"Original and compressed images must have the same dimensions.\")\n",
    "    \n",
    "    # Convert inputs to np arrays\n",
    "    original_np = np.array(original)\n",
    "    compressed_np = np.array(compressed)\n",
    "    # Calculate the Mean Squared Error\n",
    "    mse = np.mean((original_np - compressed_np) ** 2)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def calc_avg_mse(mse_values):\n",
    "    # Convert to NumPy array if it's a list\n",
    "    mse_array = np.array(mse_values)\n",
    "    \n",
    "    # Calculate the average MSE\n",
    "    average_mse = np.mean(mse_array)\n",
    "    \n",
    "    return average_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_DLV3():\n",
    "    for q in range(0, 101, step_size):\n",
    "        # load best instance of model\n",
    "        model_path = \"{}{}{}{}{}{}\".format(\"models\",os.sep,\"best_models_DLV3\",os.sep,q,\".pth\")\n",
    "        model_DLV3 = torch.load(model_path)\n",
    "        model_DLV3.eval()\n",
    "\n",
    "        # Define the criterion and metric for training the model.\n",
    "        criterion = utils.losses.DiceLoss()\n",
    "        metric = [utils.metrics.IoU()] \n",
    "\n",
    "        # define evaluation epochs\n",
    "        eval_epoch = utils.train.ValidEpoch(\n",
    "            model=model_DLV3,            # Model to be evaluated\n",
    "            loss=criterion,         # Loss function\n",
    "            metrics=metric,        # List of metrics\n",
    "            device=DEVICE,          # Device (CPU or GPU)\n",
    "            verbose=True            # Print evaluation progress\n",
    "        )\n",
    "\n",
    "        base_test_dir = \"{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"TestCompressed\")\n",
    "        cor_mse = {} # quality:MSE dictionary\n",
    "        IOU_scores ={} # quality: IOU dictionary\n",
    "        for i in range(0, 101, 1):\n",
    "            comp_test_dir = os.path.join(base_test_dir,str(i))\n",
    "            print(comp_test_dir)\n",
    "            # create test dataset\n",
    "            comp_test_dataset = Excavators(\n",
    "                comp_test_dir,  # Directory containing test images\n",
    "                test_masks_directory,  # Directory containing test masks\n",
    "                labels_directory,\n",
    "                augmentation=get_validation_augmentation(),  # Apply validation-specific augmentations\n",
    "                preprocessing=get_preprocessing(preprocessing_fn)  # Preprocessing function for normalization\n",
    "                )\n",
    "            mse_values =[]\n",
    "            for j in range(len(comp_test_dataset)):\n",
    "                comp_image,_ = comp_test_dataset[j]\n",
    "                original_image, _ = test_dataset[j]\n",
    "                mse = calculate_mse(original_image, comp_image)\n",
    "                mse_values.append(mse)\n",
    "            cor_mse[i] = calc_avg_mse(mse_values)\n",
    "            print('MSE: ', cor_mse[i])\n",
    "\n",
    "            \n",
    "\n",
    "            test_loader = DataLoader(comp_test_dataset)\n",
    "            eval_logs = eval_epoch.run(test_loader)\n",
    "            eval_loss, eval_metric_IOU = list(eval_logs.values())\n",
    "            IOU_scores[i] = eval_metric_IOU\n",
    "\n",
    "            # Print evaluation results\n",
    "            print(f\"Evaluation Loss: {eval_loss}\")\n",
    "            print(f\"Evaluation IoU: {eval_metric_IOU}\")  \n",
    "\n",
    "        mse_iou_dictionary = {value: IOU_scores[key] for key, value in cor_mse.items()}\n",
    "        # Sort the dictionary by its keys (x values) in ascending order\n",
    "        sorted_mse_iou_dict = {k: v for k, v in sorted(mse_iou_dictionary.items())}\n",
    "\n",
    "        # create a graph\n",
    "        x1 = list(sorted_mse_iou_dict.keys()) # MSE VALUES\n",
    "        y1 = list(sorted_mse_iou_dict.values()) # IOU VALUES\n",
    "        plt.figure(figsize=(7, 5)) # Set the figure size\n",
    "        plt.ylim(0, 1)\n",
    "        plt.plot(x1, y1)  # Plot with markers\n",
    "        plt.xlabel('Mean-Squared-Error')  # Label for X-axis\n",
    "        plt.ylabel('IOU')  # Label for Y-axis\n",
    "        plt.title(f'MSE-IOU Graph using a DLV3 model trained on a dataset of quality ={q}')  # Title of the graph\n",
    "        plt.grid(True)  # Show grid\n",
    "        plt.show() \n",
    "\n",
    "        # plot quality over IOU\n",
    "        x2 = list(IOU_scores.keys()) # quality VALUES\n",
    "        y2 = list(IOU_scores.values()) # IOU VALUES\n",
    "        plt.figure(figsize=(7, 5)) # Set the figure size\n",
    "        plt.plot(x2, y2)  # 'marker='o'' can be removed if you don't want the dots\n",
    "        plt.xlabel('Quality Factor')\n",
    "        plt.ylabel('IOU')\n",
    "        plt.title(f'IOU-Quality using a DLV3 model trained on a dataset of quality ={q}')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Test on normal dataset\n",
    "        test_data_directory = \"{}{}{}{}{}{}\".format(\"excavator_dataset_w_masks2\",os.sep,\"dataset\",os.sep,\"Test\",os.sep)\n",
    "        normal_test_dataset = Excavators(\n",
    "            test_data_directory,  # Directory containing test images\n",
    "            test_masks_directory,  # Directory containing test masks\n",
    "            labels_directory,\n",
    "            augmentation=get_validation_augmentation(),  # Apply validation-specific augmentations\n",
    "            preprocessing=get_preprocessing(preprocessing_fn)  # Preprocessing function for normalization\n",
    "            )\n",
    "        normal_test_loader = DataLoader(normal_test_dataset)\n",
    "        eval_logs = eval_epoch.run(normal_test_loader)\n",
    "        eval_loss, eval_metric_IOU = list(eval_logs.values())\n",
    "\n",
    "        # Print evaluation results\n",
    "        print(f\"DLV3 model trained on dataset of quality= {q} \\ntest on uncompressed dataset\")\n",
    "        print(f\"Evaluation Loss: {eval_loss}\")\n",
    "        print(f\"Evaluation IoU: {eval_metric_IOU}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excavator_dataset_w_masks2\\dataset\\TestCompressed\\0\n",
      "Loaded 51 images from excavator_dataset_w_masks2\\dataset\\TestCompressed\\0\n",
      "Loaded 51 masks from excavator_dataset_w_masks2\\dataset\\Testannot\\\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_DLV3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 37\u001b[0m, in \u001b[0;36mtest_DLV3\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m mse_values \u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(comp_test_dataset)):\n\u001b[1;32m---> 37\u001b[0m     comp_image,_ \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_test_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     38\u001b[0m     original_image, _ \u001b[38;5;241m=\u001b[39m test_dataset[j]\n\u001b[0;32m     39\u001b[0m     mse \u001b[38;5;241m=\u001b[39m calculate_mse(original_image, comp_image)\n",
      "Cell \u001b[1;32mIn[3], line 274\u001b[0m, in \u001b[0;36mExcavators.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    271\u001b[0m lower \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(color, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# Create a mask for the current class\u001b[39;00m\n\u001b[1;32m--> 274\u001b[0m class_mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# Convert to binary mask (0 or 1 values)\u001b[39;00m\n\u001b[0;32m    277\u001b[0m class_mask \u001b[38;5;241m=\u001b[39m class_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\hal3k\\deeplabv3segmentation\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2416\u001b[0m, in \u001b[0;36m_all_dispatcher\u001b[1;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2325\u001b[0m \u001b[38;5;124;03m    Test whether any array element along a given axis evaluates to True.\u001b[39;00m\n\u001b[0;32m   2326\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2410\u001b[0m \n\u001b[0;32m   2411\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39mlogical_or, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out,\n\u001b[0;32m   2413\u001b[0m                           keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m-> 2416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_all_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   2417\u001b[0m                     where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, where, out)\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_all_dispatcher)\n\u001b[0;32m   2422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_DLV3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
